<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Classification of customer complaints using tensorflow Text Classification with Word Embeddings</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="css/clean-blog.min.css" rel="stylesheet">
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125724120-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125724120-1');
</script>


  </head>

  <body>

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand" href="index.html">Posts</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fa fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="index.html">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="about.html">About</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="post.html">Current Post</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="contact.html">Contact</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>
    
    
    <!-- Page Header -->
    
    
    <div id="ner"></div>
    <header class="masthead" style="background-image: url('img/eugenio-mazzone-190204-unsplash.jpg')">
      <div class="overlay"></div>
      <div class="ner">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <div class="post-heading">
              <h1>Name Entity Recognition With Stanford NLP NER Package:</h1>
              <h2 class="subheading">Automated Information Extraction from Text</h2>
              <span class="meta">Posted by
                <a href="#">Albert Opoku</a>
                on July 20, 2019</span>
            </div>
          </div>
        </div>
      </div>
    </header>

    <!-- Post Content -->
    <article>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <p>In this short post, I will show how to get the entities in an article or any text documents using Natural language 
              processing technique. We will use the powerful NER Package by Stanford NLP in this tutorial.</p>

            <p>
              Entities could be a person, organization, location, date/time, money, percentage, the list goes on. 
              This is useful if you quickly need to gather the specific salient information about a very long document, 
              example who contacted who at what time and at what place; 
              and which organization do they work for or are they discussing? Was money involved in the dealings and how much?
            </p>
           
            <p>
            If your job involve answering these kinds of questions using text documents, then this tutorial may improve your productivity.
            </p>
            <p>
             We are using the text from this article from techrunch. <a href="https://techcrunch.com/2019/06/21/african-fintech-dominates-catalyst-funds-2019-startup-cohort/">African fintech dominates Catalyst Fund’s 2019 startup cohort</a>
            

            </p>
<p>Make sure you have java.exe in this location. Sometimes, it is not required, depending on your machine setup</p>

<pre class="pre-scrollable">
import os
java_path = "C:/Program Files/Java/jre1.8.0_211/bin/java.exe"
os.environ['JAVAHOME'] = java_path
</pre>
            
<p>We import packages and create a function to capture the entities, credit to Omar Bahareth.</p>
<pre class="pre-scrollable">
from nltk.tag.stanford import StanfordNERTagger
from nltk.tokenize import word_tokenize

def formatted_entities(classified_paragraphs_list):
    entities = {'persons': list(), 'organizations': list(), 'locations': list(), 'dates': list(), 'money': list(), 'percent': list()}

    for classified_paragraph in classified_paragraphs_list:
        for entry in classified_paragraph:
            entry_value = entry[0]
            entry_type = entry[1]

            if entry_type == 'PERSON':
                entities['persons'].append(entry_value)

            elif entry_type == 'ORGANIZATION':
                entities['organizations'].append(entry_value)

            elif entry_type == 'LOCATION':
                entities['locations'].append(entry_value)
            elif entry_type == 'DATE':
                entities['dates'].append(entry_value)
            elif entry_type == 'MONEY':
                entities['money'].append(entry_value)
            elif entry_type == 'PERCENT':
                entities['percent'].append(entry_value)
    return entities

</pre>
      
<p>
Download stanford-ner-2018-10-16 and unzip to get  english.muc.7class.distsim.crf.ser.gz and stanford-ner.jar files. Save them in the appropraite folder
from stanfor nlp site <a href="https://nlp.stanford.edu/software/stanford-parser-full-2018-10-17.zip">Stanford parser</a> and run the code below.
</p>

<pre class="pre-scrollable">
tagger = StanfordNERTagger('/Users/Shared/stanford-ner/classifiers/english.muc.7class.distsim.crf.ser.gz',
               '/Users/Shared/stanford-ner/stanford-ner.jar',encoding='utf-8')
</pre>
<p>Import spacy to extract the data from the text file</p>
<pre class="pre-scrollable">
import spacy,en_core_web_sm
import pandas as pd

nlp = en_core_web_sm.load()
doc = nlp(open('African_Fintech.txt', encoding="utf8").read())
</pre>

<p>We create a dataframe that has the tokenized sentences in a column.</p>
<pre class="pre-scrollable">
 
d = []
for idno, sentence in enumerate(doc.sents):
    d.append({"id": idno, "sentence":str(sentence)})

df = pd.DataFrame(d)
df.set_index('id', inplace=True)

print('There are {}'.format(len(d)) ,'Sentences in this article')
df.head()
</pre>
<p>
Convert the sentences to a list and create the result: a dictionary of all the entities you specified above.
</p>
            
<pre class="pre-scrollable">
df1=df.sentence.tolist()
tokenized_paragraphs = list()

for text in df1:
    tokenized_paragraphs.append(word_tokenize(text))

classified_paragraphs_list = tagger.tag_sents(tokenized_paragraphs)


formatted_result = formatted_entities(classified_paragraphs_list)
print(formatted_result)
</pre>

<a href="#">
              <img class="img-fluid" src="img/Capture.JPG" alt="">
            </a>


<p>The complete code is saved on <a href="https://github.com/opokualbert/Name_Entity_Recognition_With_Stanford_NLP/blob/master/Name_Entity_Recognition_With_Stanford_NLP.ipynb">Github</a>.</p>

         </div>
        </div>
      </div>
    </article>

    <hr>
    
    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <ul class="list-inline text-center">
              <li class="list-inline-item">
                <a href="https://twitter.com/opalbert">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://www.linkedin.com/in/albertopokupredictiveanalytics">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://github.com/opokualbert">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
            </ul>
            <p class="copyright text-muted">Copyright &copy; Albert Opoku 2019</p>
          </div>
        </div>
      </div>
    </footer>
 <hr>
    
    <!-- Page Header -->
    <div id="complaint"></div>
    <header class="masthead" style="background-image: url('img/eugenio-mazzone-190204-unsplash.jpg')">
      <div class="overlay"></div>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <div class="post-heading">
              <h1>Classification of Customer Complaints using Tensorflow:</h1>
              <h2 class="subheading">Text Classification with Word Embeddings</h2>
              <span class="meta">Posted by
                <a href="#">Albert Opoku</a>
                on September 12, 2018</span>
            </div>
          </div>
        </div>
      </div>
    </header>

    <!-- Post Content -->
    <article>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <p>In this post, I show how to classify consumer complaints text into these categories: Debt collection, Consumer Loan, Mortgage, Credit card, Credit reporting, Student loan, Bank account or service, Payday loan, Money transfers, Other financial service, Prepaid card.</p>

            <p>This kind of model will be very useful for a customer service department that wants to classify the complaints they receive from their customers. The classification of the issues they have received into buckets will help the department to provide customized solutions to the customers in each group.</p>
           
            <p>This model can also be expanded into a system, that can recommend automatic solutions to future complaints as they come in. In the past, performing these kinds of tasks were done manually by multiple employees and of course, take a long time to accomplish, delaying swift response to the complaints received.</p>
            <p>Machine learning and AI are here to solve this caliber of problems. Imagine you can classify new complaints with 95% accuracy and route them to the right team to resolve the issue. That will be a win and time saving to any business. Your customers will be happy because the right expert from your business will talk to your customers in trying to resolving their complaints. This will translate into lowering churning rate which means more revenue.</p>
            <p>I trained a text classifier with 66,806 of data on customers that have made a complaint to consumer financial protection bureau - CFPB about US financial institutions on the services they have rendered to these consumers. The dataset is on kaggle.com at this link <a href="https://www.kaggle.com/cfpb/us-consumer-finance-complaints"></a>.</p> 
            <p>I used the universal-sentence-encoder-large/3 module on the new tensorflowhub platform to leverage the power of transfer learning which according to Wikipedia, is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. Google and other teams have made available on tensorflowhub, models that took them about 62,000 GPU hours to train for our free use.</p>
            <p>This dataset is relatively not large but this kind of machine learning process requires more compute power so I chose to use Google’s colab, which gives the option to train a model with free GPU. I have a previous blog post on downloading Kaggle datasets into Google Colab on my <a href="https://opokualbert.com/post.html#colab">website</a>, you may want to check it out if you are interested in downloading this dataset to follow along with this demo.  
            <p>I will walk through the steps and in the end, we will classify new complaints and see how the model performed.</p>
            

            <p>We import the needed packages for this work.</p>
<pre class="pre-scrollable">
import os
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import json
import pickle
import urllib

from sklearn.preprocessing import LabelBinarizer

print(tf.__version__)</pre>
  <h2 class="section-heading">Data Preparation and Cleaning</h2>      
            <p>Take a look at the data with pandas.</p>
<pre class="pre-scrollable">
data = data[pd.notnull(data['consumer_complaint_narrative'])]
data.head()
</pre>
            <p>We convert the consumer_complaint_narrative column to lower case, perfect for NLP.</p>
<pre class="pre-scrollable">
pd.set_option('max_colwidth', 1000)
data['consumer_complaint_narrative'] = data['consumer_complaint_narrative'].str.lower()
data.head()</pre>

 <p>We remove characters that do not have predictive power and can misinform the model.</p>
<pre class="pre-scrollable">
 import re
data['consumer_complaint_narrative'] = data['consumer_complaint_narrative'].str.replace('x', '')
data['consumer_complaint_narrative'] = data['consumer_complaint_narrative'].str.replace('{', '')
data['consumer_complaint_narrative'] = data['consumer_complaint_narrative'].str.replace('}', '')
data['consumer_complaint_narrative'] = data['consumer_complaint_narrative'].str.replace('/', '')
data.head() </pre>
            <p>We assign the the complaint text and the product type to variables to be able to preprocess. We also set aside 0.0001 for testing after we train the model. The remainder of the data is split into train and validation sets.</p>
            
<pre class="pre-scrollable">
data_comp=data[['consumer_complaint_narrative']]
data_prod=data[['product']]

train_size = int(len(data_comp) * .999)
train_descriptions = data_comp[:train_size].astype('str')
train_prod = data_prod[:train_size]
test_descriptions = data_comp[train_size:].astype('str')
test_prod =data_prod[train_size:]

train_size = int(len(train_descriptions) * .8)
train_desc = train_descriptions[:train_size]
train_pr = train_prod[:train_size]
val_desc = train_descriptions[train_size:]
val_pr =train_prod[train_size:]
</pre>

            <p>We will use scikit learn to encode the labels into one-hot vector. We also print encoder.classes_ to show the list of all the classes the model will be predicting.</p>
              
<pre class="pre-scrollable">
from sklearn import preprocessing
encoder = preprocessing.LabelBinarizer()
encoder.fit_transform(train_pr)
train_encoded = encoder.transform(train_pr)
val_encoded = encoder.transform(val_pr)
num_classes = len(encoder.classes_)

# Print all possible products and the label for the first complaint in our training dataset
print(encoder.classes_)
print(train_encoded[0])
</pre>
            <h2 class="section-heading">Model training using TensorFlow DNNEstimator</h2>
            
            <p>We will download the tfhub pre-trained text embeddings universal-sentence-encoder-large/3 to encode the complaint text into high dimensional text vectors. Be aware that this can take a few minutes to download.</p>
<pre class="pre-scrollable">
description_embeddings = hub.text_embedding_column("descriptions", module_spec="https://tfhub.dev/google/universal-sentence-encoder-large/3", trainable=False)
</pre>          
            <p>We are going to use a DNNEstimator to train a deep neural net.The model will have 2 hidden layers of 64 neurons in the first layer and 10 neurons in the second layer. We also use a batch_size of 100 and 10 epochs. These are parameters you can tune to find the perfect combination. We also pass the inputs, in this case, the features and label as well as the description_embeddings.</p>
            <p>Batch_size represent the number of examples that will be passed to our model during one iteration, and num_epochs is the number of times our model will go through the entire training set.</p>
<pre class="pre-scrollable">

multi_label_head  = tf.contrib.estimator.multi_label_head(
    num_classes,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE
)
features = {
  "descriptions": np.array(train_desc).astype(np.str)
}
labels = np.array(train_encoded).astype(np.int32)
train_input_fn = tf.estimator.inputs.numpy_input_fn(features, labels, shuffle=True, batch_size=100, num_epochs=10)
estimator = tf.contrib.estimator.DNNEstimator(
    head=multi_label_head,
    hidden_units=[64,10],
    feature_columns=[description_embeddings])    
 </pre>
 <p>We are ready to train the model. We will time it to know how long it took to finish training.</p>
<pre class="pre-scrollable">  
%%timeit
estimator.train(input_fn=train_input_fn)

#This is optional to run to get the training accuracy so that we can compare with the validation accuracy to check for overfitting
%%timeit
train_input_fn_1 = tf.estimator.inputs.numpy_input_fn({"descriptions": np.array(train_desc).astype(np.str)}, train_encoded.astype(np.int32), shuffle=False)
estimator.evaluate(input_fn=train_input_fn_1)
</pre>
 
            <p>It is time to evaluate our model with the evaluation dataset we set aside. The results is pretty good at 95%. You can tune the hyperparameters to acheive a better results.</p>
<pre class="pre-scrollable">

eval_input_fn = tf.estimator.inputs.numpy_input_fn({"descriptions": np.array(val_desc).astype(np.str)}, val_encoded.astype(np.int32), shuffle=False)
estimator.evaluate(input_fn=eval_input_fn)
</pre>
            <p>Finally, let us test our model and see how it will do on classifying new data. We will test the model with 67 records. It got a few incorrect. You will see that those the model could not predict correctly also had low confidence values.</p>
 <pre class="pre-scrollable">
 
predict_input_fn = tf.estimator.inputs.numpy_input_fn({"descriptions": np.array(test_descriptions).astype(np.str)}, shuffle=False)
results = estimator.predict(predict_input_fn)
for product in results:
  top = product['probabilities'].argsort()[-1:]
  for prod in top:
    text_prod = encoder.classes_[prod]
    print(text_prod + ': ' + str(round(product['probabilities'][prod] * 100, 2)) + '%')
  print('')
  </pre>
            <p>The complete code is saved on <a href="https://github.com/opokualbert/Classification-of-customer-complaints-using-tensorflow---Text-Classification-with-Word-Embeddings/blob/master/Consumer%20Complaints%20Classification.ipynb">Github</a>.</p>

         </div>
        </div>
      </div>
    </article>

    <hr>
    
    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <ul class="list-inline text-center">
              <li class="list-inline-item">
                <a href="https://twitter.com/opalbert">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://www.linkedin.com/in/albertopokupredictiveanalytics">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://github.com/opokualbert">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
            </ul>
            <p class="copyright text-muted">Copyright &copy; Albert Opoku 2018</p>
          </div>
        </div>
      </div>
    </footer>
 <hr>
    
    
    
    <div id="colab"></div>
    <header class="masthead" style="background-image: url('img/colab.png')">
      <div class="overlay"></div>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <div class="post-heading">
              <h1>Downloading Kaggle Datasets into Google Colab</h1>
              <h2 class="subheading">Easy Access to Kaggle Datasets in Colab</h2>
              <span class="meta">Posted by
                <a href="#">Albert Opoku</a>
                on September 3, 2018</span>
            </div>
          </div>
        </div>
      </div>
    </header>

    <!-- Post Content -->
    <article>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <p>In this tutorial, I show how to download kaggle datasets into google colab. Kaggle has been and remains the de factor platform to try your hands on data science projects. The platform has huge rich free datasets for machine learning projects.</p>

            <p>Another product from google, the company behind kaggle is colab, a platform suitable for training machine learning models and deep neural network free of charge without any installation requirement. One key thing that makes colab a game changer, especially for people who do not own GPU laptop is that users have the option to train their models with free GPU. Colab does not have the trove of datasets kaggle host on its platform therefore, it will be nice if you could access the datasets on kaggle from colab. There is in fact a kaggle API which we can use in colab but setting it up to work is not so easy. I would want to show how to use the API in a few simple steps.</p>
           
            <h2 class="section-heading">Step 1</h2>
            <p>Create a kaggle account if you do not have one already. Click on your user name, click on account.</p>
             <a href="#">
              <img class="img-fluid" src="img/kaggle api 1.png" alt="">
            </a>
            <p>Scroll down to click on create new API token. This will download a file unto your PC. Note the location of the downloaded file.</p>
            <a href="#">
              <img class="img-fluid" src="img/kaggle api 2.PNG" alt="">
            </a>

            <h2 class="section-heading">Step 2</h2>
            <p>Go to colab via this link: <a href="https://colab.research.google.com/notebooks/welcome.ipynb">Colab</a> and under file, click on new python 3 notebook. In the first cell, type this code to install kaggle API and make a directory called kaggle.</p>

              <pre class="pre-scrollable">
!pip install -U -q kaggle
!mkdir -p ~/.kaggle</pre>
            <h2 class="section-heading">Step 3</h2>

            <p>Type this code into the next cell and run to import the API key into colab</p>
            <pre class="pre-scrollable">
from google.colab import files
files.upload()</pre>
            <p>In the next cell, run this code to copy the API key to the kaggle directory we created.</p>
<pre class="pre-scrollable">
!cp kaggle.json ~/.kaggle/</pre>            


            <p>The datasets should be available for us to use. Let us list the datasets with this code.</p>
<pre class="pre-scrollable">
!kaggle datasets list</pre>
            
            <h2 class="section-heading">Step 4</h2>

            <p>We can download files now by using this sample code. In this case the US consumer finance complaints was downloaded.</p>
<pre class="pre-scrollable">
!kaggle datasets download -d cfpb/us-consumer-finance-complaints
!ls</pre>
            <h2 class="section-heading">Step 5</h2>
            <p>We use pandas to read the data we have downloaded by unzipping the file first. This line of code works in most situations.</p>
<pre class="pre-scrollable">
import pandas as pd
data2 = pd.read_csv('/content/us-consumer-finance-complaints.zip', compression='zip', header=0, sep=',', quotechar='"')</pre>
            <p>It did not work here because the zipped file also contains a sqlite database. I will use a different method below to extract only the CSV.</p>
<pre class="pre-scrollable">
from zipfile import ZipFile
zip_file = ZipFile('/content/us-consumer-finance-complaints.zip')
fields= ['product','consumer_complaint_narrative'] 
data=pd.read_csv(zip_file.open('consumer_complaints.csv'), usecols=fields)
data.head()</pre>
            
            <p>You can get the entire code on at <a href="https://github.com/opokualbert/Downloading-Kaggle-Datasets-into-Google-Colab">GitHub</a>.</p>
              
         </div>
        </div>
      </div>
    </article>

    <hr>
    
    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <ul class="list-inline text-center">
              <li class="list-inline-item">
                <a href="https://twitter.com/opalbert">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://www.linkedin.com/in/albertopokupredictiveanalytics">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://github.com/opokualbert">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
            </ul>
            <p class="copyright text-muted">Copyright &copy; Albert Opoku 2018</p>
          </div>
        </div>
      </div>
    </footer>
 <hr>

    
    
    <div id="Soccer"></div>
    <header class="masthead" style="background-image: url('img/Player.jpg')">
      <div class="overlay"></div>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <div class="post-heading">
              <h1>Exploring the Causes of Death of Soccer Players</h1>
              <h2 class="subheading">SPARQL and Python Tutorial</h2>
              <span class="meta">Posted by
                <a href="#">Albert Opoku</a>
                on July 14, 2018</span>
            </div>
          </div>
        </div>
      </div>
    </div>
    </header>
     <!-- Post Content -->
    <article>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <p>As we watch soccer players exhibit their skills on the pitch at the world cup stage, we would think these players are healthy in all sense, given the amount of work they put in before and during each game. Health experts advise, exercising is paramount for avoiding many diseases. In that context, I wondered what is the impact of exercise on sports men and women, and in particular, soccer players.</p>

            <p>These questions cannot be answered for players currently playing in the game because I do not have access to their medical records so I decided to use a public data on Wikipedia which show the causes of death of soccer players on earth.</p>

            <p>I decided to make this more of a tutorial so I will share steps of the data acquisition through to the analysis. I wanted to find out the average age at death, and the causes of death of soccer players that have passed away.</p>
            

            <h2 class="section-heading">Load The Required Python Packages</h2>

              <pre class="pre-scrollable">
#%reload_ext signature 
# To sign my name at the end of code .You need to have a script saved before it works. You can remove it.
%matplotlib inline
import requests
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import io
chartinfo = ‘Author: Albert Opoku• Data source: wikidata.org’
infosize = 12</pre>

            <h2 class="section-heading">Querying data from Wikipedia using SPARQL language</h2>

            <p>SPARQL developed by W3C is an RDF query language, that is, a semantic query language for databases, able to retrieve and manipulate data stored in Resource Description Framework (RDF) format. I decided to write my SPARQL code in python because python with pandas gives me the ability do my analysis without changing to another programming language.</p>

            <p>I will query the following information: Name, Country of citizenship, Cause of death, date of birth and date of death for each soccer player reported dead on Wikipedia.</p>

            <p>I would like to acknowledge Ramiro Gómez for sharing his SPARQL python code on <a href="https://github.com/opokualbert/Causes-of-Death-for-Soccer-Players/">GitHub</a>.</p>
            <!--pre class="pre-scrollable"-->
            
query = '''PREFIX wikibase: <http://wikiba.se/ontology#>
PREFIX wd: <http://www.wikidata.org/entity/>
PREFIX wdt: <http://www.wikidata.org/prop/direct/>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
SELECT ?name ?cause ?dob ?dod ?country WHERE {
 ?pid wdt:P106 wd:Q937857 . 
 ?pid wdt:P509 ?cid .
 ?pid wdt:P569 ?dob .
 ?pid wdt:P570 ?dod .
 ?pid wdt:P27 ?cot.
OPTIONAL {
 ?pid rdfs:label ?name filter (lang(?name) = “en”) .
 }
 OPTIONAL {
 ?cid rdfs:label ?cause filter (lang(?cause) = “en”) .
 }
 OPTIONAL {
 ?cot rdfs:label ?country filter (lang(?country) = “en”) .
 }
}'''
url = ‘https://query.wikidata.org/bigdata/namespace/wdq/sparql'
data = requests.get(url, params={‘query’: query, ‘format’: ‘json’}).json()
 <!--/pre-->
 
            <p>I will use the code below to parse the values we need out of the json and save in a pandas DataFrame for analysis. There are 1,390 soccer players reported dead on Wikipedia at the time of writing.</p>
<pre class="pre-scrollable">
soccer = []
for item in data['results']['bindings']:
    try:
        soccer.append({
        'name': item['name']['value'],
        'country': item['country']['value'],
        'cause_of_death': item['cause']['value'],
        'date_of_birth': item['dob']['value'],
        'date_of_death': item['dod']['value']})
    except KeyError:
        pass
df = pd.DataFrame(soccer)
print(len(df))
df.head()
</pre>
         <p>Let us use df.info() to check the data types and also find out if there are missing values in any of the columns. We will also use the df.describe() to get the summary statistics on the data. Turns out the fields are all object type and there are no missing values.</p> 
   <pre class="pre-scrollable">
df.info()
df.describe()
</pre>
        <p>Let us extract the year out of the date of birth and date of death fields so that we can calculate the age of each player.</p>
<pre class="pre-scrollable">
df[‘date_of_death’] = df.date_of_death.str.slice(0, 4).astype(int)
df[‘date_of_birth’] = df.date_of_birth.str.slice(0, 4).astype(int)
df['age']=df.date_of_death - df.date_of_birth
print(df.head())
df.describe()
</pre>
       <p>It does look like Jackie Benyon was born in the year 2000 but died in 1937. How could that be? That is what you deal with when you work with raw data. We will exclude Jackie Benyon from the data soon. Data on Xie Hui also doesn’t seem to be correct.</p>
<pre class="pre-scrollable">  
df[(df['date_of_death']==426)|(df['age']==-63)]
df=df[df['age']!=-63]
df=df[df['date_of_death']!=426]
df.describe()  
</pre>

      <p>After excluding these two players, the average age at death for soccer players is 55.Now let us visualize the causes of death for the players. I will visualize the top 20 causes. It is worth to note that Wikipedia sometimes publish multiple cause of death for an individual. It could be that doctors attribute multiple diseases as the cause of death for some patients.</p>
 <pre class="pre-scrollable">
title = 'Association Football(Soccer) players top 20 Causes of Death According to Wikipedia'
footer =  chartinfo
    
df['cause of death'] = df['cause_of_death'].apply(lambda x: x.capitalize())
s = df.groupby('cause_of_death').agg('count')['name'].sort_values().tail(20)
ax = s.plot(kind='barh', figsize=(9, 8), title=title,color='xkcd:sky blue')
ax.yaxis.set_label_text('')
ax.xaxis.set_label_text('Cause of death count')
ax.annotate(footer, xy=(-0.1, -0.14), xycoords='axes fraction', fontsize=infosize)
plt.savefig('img',dpi=300, bbox_inches = "tight")
</pre>

<img class="img-fluid" src="img/img_players_graph.png" alt="">
         <p>The result is not as expected. You would think people that workout that much won’t develop myocardial infarction aka “heart attack”. Perhaps they stop exercising when they retired from active football or food choices may be blamed. The doctors can help us out. Traffic accident at position number three is interesting. I can’t think about any reason behind this, there surely maybe something going on with these players that is not captured in this data. Have fun, explore the rest of the graph. You can also try your hands on the code.</p>
  
         </div>
        </div>
      </div>
    </article>

    <hr>
    
    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <ul class="list-inline text-center">
              <li class="list-inline-item">
                <a href="https://twitter.com/opalbert">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://www.linkedin.com/in/albertopokupredictiveanalytics">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://github.com/opokualbert">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
            </ul>
            <p class="copyright text-muted">Copyright &copy; Albert Opoku 2018</p>
          </div>
        </div>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/clean-blog.min.js"></script>

  </body>

</html>
